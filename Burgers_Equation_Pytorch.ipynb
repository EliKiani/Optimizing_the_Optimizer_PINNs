{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad85781f-ea49-4a84-a9bf-753063e6b6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cholesky, LinAlgError\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "RESULTS_DIR = \"results2\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0be838-e5bb-4543-bfc1-24b237526734",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = 1  \n",
    "L1 = 20\n",
    "layer_dims = (2, L1, L1, L1, L1, 1)\n",
    "\n",
    "L = 2\n",
    "x0 = -1\n",
    "xf = x0 + L\n",
    "tfinal = 1\n",
    "nu = 0.01 / np.pi\n",
    "\n",
    "\n",
    "Nepochs_ADAM  = 1000\n",
    "Nchange  = 500\n",
    "Nint     = 8000\n",
    "N0       = 500\n",
    "Nb       = 500\n",
    "Nprint   = 100\n",
    "\n",
    "k1 = 1  \n",
    "k2 = 1  \n",
    "rad_args = (k1, k2) \n",
    "\n",
    "\n",
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * scaling\n",
    "\n",
    "\n",
    "def init_variance_scaling_fan_avg_uniform(linear, scale):\n",
    "    fan_in, fan_out = linear.in_features, linear.out_features\n",
    "    n = 0.5 * (fan_in + fan_out)\n",
    "    limit = math.sqrt(3.0 * scale / n)\n",
    "    with torch.no_grad():\n",
    "        linear.weight.uniform_(-limit, limit)\n",
    "        if linear.bias is not None:\n",
    "            linear.bias.zero_()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(layer_dims[0], layer_dims[1]), nn.Tanh(),\n",
    "            nn.Linear(layer_dims[1], layer_dims[2]), nn.Tanh(),\n",
    "            nn.Linear(layer_dims[2], layer_dims[3]), nn.Tanh(),\n",
    "            nn.Linear(layer_dims[3], layer_dims[4]), nn.Tanh(),\n",
    "            nn.Linear(layer_dims[4], layer_dims[5]),\n",
    "            CustomActivation(),)\n",
    "        final_linear = self.net[-2]\n",
    "        init_variance_scaling_fan_avg_uniform(final_linear, scale=1.0/(scaling**2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "N = Net()\n",
    "\n",
    "def generate_inputs(Nint):\n",
    "    t = tfinal * np.random.rand(Nint)\n",
    "    x = L * np.random.rand(Nint) + x0\n",
    "    X = np.hstack((t[:, None], x[:, None]))\n",
    "    print('x', x.shape, 't', t.shape, 'X', X.shape)\n",
    "    return torch.as_tensor(X)\n",
    "\n",
    "def initial_points(N0):\n",
    "    t = np.zeros(N0)\n",
    "    x = L * np.random.rand(N0) + x0\n",
    "    X = np.hstack((t[:, None], x[:, None]))\n",
    "    return torch.as_tensor(X)\n",
    "\n",
    "def boundary_points(Nb):\n",
    "    t = tfinal * np.random.rand(Nb)\n",
    "    x = L * np.random.randint(0, 2, size=(Nb,)) + x0\n",
    "    X = np.hstack((t[:, None], x[:, None]))\n",
    "    return torch.as_tensor(X)\n",
    "\n",
    "def generate_validation(Nt, Nx):\n",
    "    x = np.linspace(x0, xf, Nx)\n",
    "    t = np.linspace(0, tfinal, Nt)\n",
    "    x, t = np.meshgrid(x, t)\n",
    "    X = np.hstack((t.flatten()[:, None], x.flatten()[:, None]))\n",
    "    print('x', x.shape, 't', t.shape, 'X', X.shape)\n",
    "    return torch.as_tensor(X), t, x\n",
    "\n",
    "def adaptive_rad(N, Nint, rad_args, Nsampling=50000):\n",
    "    Xtest = generate_inputs(Nsampling)              \n",
    "    k1, k2 = rad_args\n",
    "    Y = torch.abs(get_results(N, Xtest)[-1]).reshape(-1)  \n",
    "    w = (Y**k1)\n",
    "    err_eq = w / w.mean() + k2\n",
    "    p = (err_eq / err_eq.sum()).clamp_min(1e-12)     \n",
    "    X_ids = torch.multinomial(p, num_samples=Nint, replacement=False)\n",
    "    return Xtest[X_ids]\n",
    "\n",
    "def uinit(X): \n",
    "    x = X[:, 1:2]\n",
    "    return -torch.sin(torch.pi * x)\n",
    "\n",
    "def uleft(X): \n",
    "    Xleft = X[X[:, 1] == x0]\n",
    "    t = Xleft[:, 0]\n",
    "    return torch.zeros((t.shape[0], 1), dtype=torch.float64)\n",
    "\n",
    "def uright(X):  \n",
    "    Xright = X[X[:, 1] == xf]\n",
    "    t = Xright[:, 0]\n",
    "    return torch.zeros((t.shape[0], 1), dtype=torch.float64)\n",
    "\n",
    "def output(N, X):\n",
    "    Nout = N(X)\n",
    "    u = Nout[:, 0:1]\n",
    "    return u\n",
    "\n",
    "def get_results(N, X):\n",
    "    X.requires_grad_(True)\n",
    "    u = output(N, X)                 \n",
    "    grads = torch.autograd.grad(\n",
    "        u, X,\n",
    "        grad_outputs=torch.ones_like(u),\n",
    "        create_graph=True,\n",
    "        retain_graph=True)[0]        \n",
    "    u_t = grads[:, 0]                  \n",
    "    u_x = grads[:, 1]                  \n",
    "\n",
    "    u_xx = torch.autograd.grad(\n",
    "        u_x, X,\n",
    "        grad_outputs=torch.ones_like(u_x),\n",
    "        create_graph=True,\n",
    "        retain_graph=True)[0][:, 1]\n",
    "    fu = u_t + u[:, 0] * u_x - nu * u_xx\n",
    "    return u, fu\n",
    "\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "lam0 = 5.0\n",
    "lamB = 5.0\n",
    "\n",
    "def loss(fu, u0, u0pinn, ul, ulpinn, ur, urpinn):\n",
    "    Ntot = fu.shape[0]\n",
    "    zeros = torch.zeros((Ntot, 1), dtype=torch.get_default_dtype(), device=fu.device)\n",
    "    fu_col = fu.reshape(-1, 1)\n",
    "    loss_value = (loss_function(fu_col, zeros)\n",
    "                  + lam0 * loss_function(u0, u0pinn)\n",
    "                  + lamB * (loss_function(ul, ulpinn) + loss_function(ur, urpinn)))\n",
    "    return loss_value\n",
    "\n",
    "def grads(N, X, X0, Xb):\n",
    "    for p in N.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "    X = X.clone().detach().requires_grad_(True)\n",
    "    X0 = X0.clone().detach().requires_grad_(True)\n",
    "    _, fu = get_results(N, X)\n",
    "    u0 = uinit(X0)\n",
    "    u0pinn = output(N, X0)\n",
    "    ul = uleft(Xb)\n",
    "    ur = uright(Xb)\n",
    "    mask_l = (Xb[:, 1] == x0)\n",
    "    mask_r = (Xb[:, 1] == xf)\n",
    "    ulpinn = output(N, Xb[mask_l])\n",
    "    urpinn = output(N, Xb[mask_r])\n",
    "\n",
    "    loss_value = loss(fu, u0, u0pinn, ul, ulpinn, ur, urpinn)\n",
    "\n",
    "    loss_value.backward()\n",
    "\n",
    "    gradsN = [p.grad for p in N.parameters()]\n",
    "    return gradsN, loss_value\n",
    "\n",
    "\n",
    "def training(N, X, X0, Xb, optimizer):\n",
    "    _, loss_value = grads(N, X, X0, Xb)\n",
    "    optimizer.step()\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "X  = generate_inputs(Nint)\n",
    "X0 = initial_points(N0)\n",
    "Xb = boundary_points(Nb)\n",
    "\n",
    "\n",
    "template = 'Epoch {}, loss: {}'\n",
    "\n",
    "rel_adam = [] \n",
    "optimizer = torch.optim.Adam(N.parameters(), lr=5e-3, betas=(0.99, 0.999), eps=1e-20)\n",
    "adam_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer, lr_lambda=lambda step: 0.98 ** (step / 1000.0))\n",
    "\n",
    "adam_losses = []\n",
    "start_time = perf_counter()\n",
    "\n",
    "adam_t0 = perf_counter()\n",
    "for epoch in range(Nepochs_ADAM):\n",
    "    if (epoch + 1) % Nchange == 0:\n",
    "        X  = adaptive_rad(N, Nint, rad_args)\n",
    "        X0 = initial_points(N0)\n",
    "        Xb = boundary_points(Nb)\n",
    "\n",
    "    loss_value = training(N, X, X0, Xb, optimizer)\n",
    "    adam_losses.append(float(loss_value.detach().cpu().item()))\n",
    "\n",
    "    if (epoch + 1) % Nprint == 0:\n",
    "        print(template.format(epoch + 1, adam_losses[-1]))\n",
    "\n",
    "        N.eval()\n",
    "        with torch.no_grad():\n",
    "            u_pred = output(N, X_star).detach().cpu().numpy().reshape(u_ref.shape)\n",
    "        N.train()\n",
    "\n",
    "        num = np.linalg.norm(u_ref - u_pred)\n",
    "        den = np.linalg.norm(u_pred) + 1e-12\n",
    "        rel = num / den\n",
    "        rel_adam.append((epoch + 1, float(rel)))\n",
    "\n",
    "\n",
    "adam_time_sec = perf_counter() - adam_t0\n",
    "\n",
    "def nested_tensor(grad, layer_dims, train_activations=False, bias=True):\n",
    "    if _has_torch and isinstance(grad, torch.Tensor):\n",
    "        grad = grad.detach().cpu().numpy()\n",
    "    grad = np.asarray(grad).ravel()\n",
    "\n",
    "    if not train_activations:\n",
    "        if bias:\n",
    "            temp = [None] * (2 * len(layer_dims) - 2)  \n",
    "        else:\n",
    "            temp = [None] * (2 * len(layer_dims) - 3) \n",
    "\n",
    "        index = 0\n",
    "        for i in range(len(temp)):\n",
    "            if i % 2 == 0:\n",
    "                k = i // 2\n",
    "                fan_in, fan_out = layer_dims[k], layer_dims[k + 1]\n",
    "                expected_shape = (fan_in, fan_out)\n",
    "                size = fan_in * fan_out\n",
    "\n",
    "                print('layer_dims', layer_dims)\n",
    "                print(f\"Expected shape: {expected_shape}, Grad slice size: {size}\")\n",
    "                print(f\"Current index: {index}, Next index: {index + size}\")\n",
    "                print(f\"Size of grad slice: {grad[index:index+size].size}\")\n",
    "\n",
    "                temp[i] = grad[index:index + size].reshape(expected_shape)\n",
    "                index += size\n",
    "            else:\n",
    "                k = i - (i // 2)       \n",
    "                bsz = layer_dims[k]\n",
    "                temp[i] = grad[index:index + bsz]\n",
    "                index += bsz\n",
    "        return temp\n",
    "\n",
    "    else:\n",
    "        temp = [None] * (3 * len(layer_dims) - 4)\n",
    "        index = 0\n",
    "        for i in range(len(temp)):\n",
    "            if i % 3 == 0:\n",
    "          \n",
    "                k = i // 3\n",
    "                fan_in, fan_out = layer_dims[k], layer_dims[k + 1]\n",
    "                size = fan_in * fan_out\n",
    "                temp[i] = grad[index:index + size].reshape(fan_in, fan_out)\n",
    "                index += size\n",
    "            elif i % 3 == 1:\n",
    "\n",
    "                k = int((i + 2) / 3)  \n",
    "                bsz = layer_dims[k]\n",
    "                temp[i] = grad[index:index + bsz]\n",
    "                index += bsz\n",
    "            else:\n",
    "  \n",
    "                temp[i] = grad[index]\n",
    "                index += 1\n",
    "        return temp\n",
    "\n",
    "\n",
    "power = 1.0  \n",
    "\n",
    "def loss_and_gradient_torch(N, X, X0, Xb, power=power):\n",
    "    X = X.clone().detach().requires_grad_(True)\n",
    "    _, fu = get_results(N, X)\n",
    "    u0 = uinit(X0)\n",
    "    u0pinn = output(N, X0)\n",
    "    ul = uleft(Xb)\n",
    "    mask_l = (Xb[:, 1] == x0)\n",
    "    ulpinn = output(N, Xb[mask_l])\n",
    "    ur = uright(Xb)\n",
    "    mask_r = (Xb[:, 1] == xf)\n",
    "    urpinn = output(N, Xb[mask_r])\n",
    "    loss_value = loss(fu, u0, u0pinn, ul, ulpinn, ur, urpinn)\n",
    "    loss_root = loss_value if power == 1.0 else loss_value ** (1.0 / power)\n",
    "    params = list(N.parameters())\n",
    "    gradsN = torch.autograd.grad(\n",
    "        loss_root, params,\n",
    "        create_graph=False, retain_graph=False, allow_unused=False)\n",
    "    return loss_root, gradsN\n",
    "\n",
    "\n",
    "def loss_and_gradient(weights, N, X, X0, Xb, layer_dims=None):\n",
    "    first_param = next(N.parameters())\n",
    "    device = first_param.device\n",
    "    dtype = first_param.dtype\n",
    "    w_tensor = torch.as_tensor(weights, dtype=dtype, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vector_to_parameters(w_tensor, N.parameters())\n",
    "    loss_val, grads_list = loss_and_gradient_torch(N, X, X0, Xb, power=power)\n",
    "    grads_flat = torch.cat([g.reshape(-1) for g in grads_list])\n",
    "\n",
    "    return float(loss_val.detach().cpu().item()), grads_flat.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "Nbfgs = 10000\n",
    "Nbatches = int(round(Nbfgs / Nchange))\n",
    "Nprint = 100\n",
    "n_ckpts        = Nbfgs // Nprint                  \n",
    "warmup_steps   = len(adam_losses)                  \n",
    "epochs_bfgs    = warmup_steps + np.arange(1, n_ckpts + 1, dtype=float) * Nprint\n",
    "lossbfgs        = np.zeros(n_ckpts, dtype=float)\n",
    "validation_list = np.zeros(n_ckpts, dtype=float)\n",
    "error_list      = np.zeros(n_ckpts, dtype=float)\n",
    "time_elapsed = np.array([])  \n",
    "initial_time = perf_counter()\n",
    "\n",
    "Nt = 300\n",
    "Nx = 300\n",
    "Xtest, t, x = generate_validation(Nt, Nx)\n",
    "X0test = Xtest[:Nx]\n",
    "mask_b = (Xtest[:, 1] == x0) | (Xtest[:, 1] == xf)\n",
    "Xbtest = Xtest[mask_b]\n",
    "\n",
    "mat = scipy.io.loadmat(\"burgers_canonical.mat\")\n",
    "u_ref  = mat[\"usol\"]\n",
    "t_star = mat[\"t\"].ravel()\n",
    "x_star = mat[\"x\"].ravel()\n",
    "x_star, t_star = np.meshgrid(x_star, t_star)\n",
    "\n",
    "device = next(N.parameters()).device\n",
    "X_star = torch.as_tensor(\n",
    "    np.hstack((t_star.reshape(-1,1), x_star.reshape(-1,1))),\n",
    "    dtype=torch.get_default_dtype(),\n",
    "    device=device)\n",
    "\n",
    "\n",
    "initial_weights = parameters_to_vector([p.detach() for p in N.parameters()]).cpu().numpy()\n",
    "\n",
    "cont = 0\n",
    "def callback(*, intermediate_result):\n",
    "    global N, cont, lossbfgs, Nprint, u_ref, \\\n",
    "           x_star, X_star, Xtest, validation_list, X0test, Xbtest, error_list, power\n",
    "\n",
    "    device = next(N.parameters()).device\n",
    "    dtype  = next(N.parameters()).dtype\n",
    "\n",
    "    cont += 1\n",
    "    if (cont % Nprint) != 0:\n",
    "        return\n",
    "\n",
    "    idx = cont // Nprint - 1\n",
    "    if idx < 0 or idx >= len(lossbfgs):  # guard\n",
    "        return\n",
    "\n",
    "    loss_value = float((intermediate_result.fun) ** power)\n",
    "    lossbfgs[idx] = loss_value\n",
    "\n",
    "    _, futest = get_results(N, Xtest.to(device))\n",
    "    zeros = torch.zeros((futest.shape[0], 1), dtype=dtype, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        u_pred = output(N, X_star).reshape(x_star.shape)        \n",
    "        u_ref_t = torch.as_tensor(u_ref, dtype=dtype, device=device)\n",
    "        v1 = loss_function(u_pred, u_ref_t)\n",
    "        v2 = loss_function(futest.reshape(-1,1), zeros)\n",
    "        validation_list[idx] = (v1 + v2).item()\n",
    "\n",
    "    u_np = u_pred.detach().cpu().numpy()\n",
    "    num = np.linalg.norm(u_ref - u_np)\n",
    "    den = np.linalg.norm(u_np) + 1e-12\n",
    "    error_list[idx] = num / den\n",
    "\n",
    "    print(f\"Iteration {cont} (ckpt {idx+1}/{len(lossbfgs)}): \"\n",
    "          f\"Train {loss_value:.3e}, relL2 {error_list[idx]:.3e}\")\n",
    "\n",
    "\n",
    "method = \"BFGS\"\n",
    "method_bfgs = \"SSBroyden2\"      \n",
    "initial_scale = False      \n",
    "warmup_tag = \"warmup_adam\" \n",
    "\n",
    "os.makedirs(\"results2\", exist_ok=True)\n",
    "\n",
    "def _scipy_cb_factory():\n",
    "    class _Res:\n",
    "        __slots__ = (\"fun\",)\n",
    "        def __init__(self, fun): self.fun = fun\n",
    "    def _cb(xk):\n",
    "        f, _ = loss_and_gradient(xk, N, X, X0, Xb, layer_dims)\n",
    "        callback(intermediate_result=_Res(f))\n",
    "    return _cb\n",
    "\n",
    "H0 = np.eye(initial_weights.size, dtype=np.float64)\n",
    "\n",
    "initial_time_bfgs = perf_counter()\n",
    "\n",
    "bfgs_loss_ckpt = []\n",
    "bfgs_rel       = []\n",
    "\n",
    "bfgs_t0 = perf_counter()\n",
    "\n",
    "while cont < Nbfgs: \n",
    "    print(cont)\n",
    "    result = minimize(\n",
    "        loss_and_gradient, \n",
    "        initial_weights, \n",
    "        args=(N, X, X0, Xb, layer_dims),\n",
    "        method=method,\n",
    "        jac=True,\n",
    "        options={\n",
    "            'maxiter': Nchange,\n",
    "            'gtol': 0,\n",
    "            'hess_inv0': H0,          \n",
    "            'method_bfgs': method_bfgs,\n",
    "            'initial_scale': initial_scale },\n",
    "        tol=0,\n",
    "        callback=callback) \n",
    "\n",
    "    initial_weights = result.x\n",
    "    H0 = result.hess_inv\n",
    "    H0 = 0.5 * (H0 + H0.T)\n",
    "    \n",
    "    try:\n",
    "        cholesky(H0)\n",
    "    except LinAlgError:\n",
    "        H0 = np.eye(len(initial_weights), dtype=np.float64) \n",
    "\n",
    "    X  = adaptive_rad(N, Nint, rad_args)\n",
    "    X0 = initial_points(N0)\n",
    "    Xb = boundary_points(Nb)\n",
    "    _, fu   = get_results(N, X)\n",
    "    u0      = uinit(X0)\n",
    "    u0pinn  = output(N, X0)\n",
    "    ul      = uleft(Xb)\n",
    "    ulpinn  = output(N, Xb[Xb[:, 1] == x0])\n",
    "    ur      = uright(Xb)\n",
    "    urpinn  = output(N, Xb[Xb[:, 1] == xf])\n",
    "    loss_value = loss(fu, u0, u0pinn, ul, ulpinn, ur, urpinn)\n",
    "\n",
    "    initial_scale = False\n",
    "\n",
    "bfgs_time_sec = perf_counter() - bfgs_t0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
