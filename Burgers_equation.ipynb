{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "574683e7-c8f3-4518-b868-b5003a096502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, convert_to_tensor\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from scipy.optimize import minimize\n",
    "from scipy.linalg import cholesky, LinAlgError\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import time\n",
    "from time import perf_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "302b3a92-04cd-4e48-b9c1-9453fae6ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64') \n",
    "tf.keras.utils.set_random_seed(2) \n",
    "\n",
    "scaling = 1 \n",
    "class CustomActivation(keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(CustomActivation, self).__init__()\n",
    "        \n",
    "    def call(self, x):\n",
    "            return x*scaling\n",
    "        \n",
    "L = 2\n",
    "x0 = -1\n",
    "xf = x0 + L\n",
    "tfinal = 1\n",
    "L1 = 20\n",
    "layer_dims = (2,L1,L1,L1,L1,1) \n",
    "\n",
    "initializer = tf.keras.initializers.VarianceScaling(\n",
    "scale=1/scaling**2, mode='fan_avg', distribution='uniform')\n",
    "\n",
    "X_input = Input((2,))\n",
    "X = Dense(L1,activation=\"tanh\")(X_input)\n",
    "X = Dense(L1,activation=\"tanh\")(X)\n",
    "X = Dense(L1,activation=\"tanh\")(X)\n",
    "X = Dense(L1,activation=\"tanh\")(X)\n",
    "X = Dense(1,activation=CustomActivation(),kernel_initializer=initializer)(X)\n",
    "N = Model(inputs=X_input, outputs=X)\n",
    "\n",
    "\n",
    "\n",
    "def generate_inputs(Nint):\n",
    "    t = tfinal*np.random.rand(Nint)\n",
    "    x = L*np.random.rand(Nint) + x0\n",
    "    X = np.hstack((t[:,None],x[:,None]))\n",
    "    print('x',x.shape , 't', t.shape, 'X', X.shape)\n",
    "    return convert_to_tensor(X)\n",
    "\n",
    "def initial_points(N0): \n",
    "    \n",
    "    t = np.zeros(N0)\n",
    "    x = L*np.random.rand(N0) + x0\n",
    "    X = np.hstack((t[:,None],x[:,None]))\n",
    "    return convert_to_tensor(X)\n",
    "\n",
    "def boundary_points(Nb): \n",
    "    t = tfinal*np.random.rand(Nb)\n",
    "    x = L*np.random.randint(0,2,size=(Nb)) + x0\n",
    "    X = np.hstack((t[:,None],x[:,None]))\n",
    "    return convert_to_tensor(X)\n",
    "\n",
    "def generate_validation(Nt,Nx): \n",
    "    x = np.linspace(x0,xf,Nx)\n",
    "    t = np.linspace(0,tfinal,Nt)\n",
    "    x,t = np.meshgrid(x,t)\n",
    "    X = np.hstack((t.flatten()[:,None],x.flatten()[:,None]))\n",
    "    print('x',x.shape , 't', t.shape, 'X', X.shape)\n",
    "    return convert_to_tensor(X),t,x\n",
    "\n",
    "\n",
    "def adaptive_rad(N,Nint,rad_args,Nsampling=50000):\n",
    "    Xtest = generate_inputs(Nsampling)\n",
    "    k1,k2 = rad_args\n",
    "    Y = tf.math.abs(get_results(N,Xtest)[-1]).numpy()\n",
    "    err_eq = np.power(Y,k1)/np.power(Y,k1).mean() + k2\n",
    "    err_eq_normalized = (err_eq / sum(err_eq))\n",
    "    X_ids = np.random.choice(a=len(Xtest), size=Nint, replace=False,\n",
    "                         p=err_eq_normalized) \n",
    "    return tf.gather(Xtest,X_ids)\n",
    "\n",
    "def uinit(X): #Initial condition\n",
    "    x = X[:,1,None]\n",
    "    return -tf.math.sin(np.pi*x)\n",
    "\n",
    "def uleft(X):\n",
    "    Xleft = X[X[:,1]==x0]\n",
    "    t = Xleft[:,0]\n",
    "    return tf.zeros([len(t),1],dtype=tf.float64)\n",
    "\n",
    "def uright(X):\n",
    "    Xright = X[X[:,1]==xf]\n",
    "    t = Xright[:,0]\n",
    "    return tf.zeros([len(t),1],dtype=tf.float64)\n",
    "\n",
    "\n",
    "def output(N,X):\n",
    "    Nout = N(X)\n",
    "    u = Nout[:,0,None]\n",
    "    return u\n",
    "\n",
    "nu = 0.01/np.pi\n",
    "def get_results(N,X):\n",
    "\n",
    "   with tf.GradientTape(persistent=True, watch_accessed_variables=False) as gt1:\n",
    "      gt1.watch(X)\n",
    "\n",
    "      with tf.GradientTape(persistent=True, watch_accessed_variables=False) as gt2:\n",
    "         gt2.watch(X)\n",
    "         u = output(N,X)\n",
    "      \n",
    "      ugrad = gt2.gradient(u, X)\n",
    "      u_t = ugrad[:,0]\n",
    "      u_x = ugrad[:,1]\n",
    "      \n",
    "   u_xx = gt1.gradient(u_x, X)[:,1]\n",
    "   fu = u_t + u[:,0]*u_x - nu*u_xx\n",
    "   return u,fu\n",
    "\n",
    "\n",
    "loss_function = keras.losses.MeanSquaredError() \n",
    "lam0 = 5 \n",
    "lamB = 5 \n",
    "def loss(fu,u0,u0pinn,ul,ulpinn,ur,urpinn):\n",
    "    Ntot = fu.shape[0]\n",
    "    zeros = tf.zeros([Ntot,1],dtype=tf.float64) \n",
    "    loss_value = loss_function(fu,zeros) + lam0*loss_function(u0,u0pinn) +\\\n",
    "                 lamB*(loss_function(ul,ulpinn) + loss_function(ur,urpinn))\n",
    "    return loss_value\n",
    "\n",
    "\n",
    "def grads(N,X,X0,Xb): \n",
    "    with tf.GradientTape() as tape2:\n",
    "        _,fu = get_results(N,X)\n",
    "        u0 = uinit(X0)\n",
    "        u0pinn = output(N, X0)\n",
    "        ul = uleft(Xb)\n",
    "        ulpinn = output(N,Xb[Xb[:,1]==x0])\n",
    "        ur = uright(Xb)\n",
    "        urpinn = output(N,Xb[Xb[:,1]==xf])\n",
    "        loss_value = loss(fu,u0,u0pinn,ul,ulpinn,ur,urpinn)\n",
    "    gradsN = tape2.gradient(loss_value,N.trainable_variables)\n",
    "    return gradsN,loss_value\n",
    "\n",
    "@tf.function(jit_compile=True) \n",
    "def training(N,X,X0,Xb,optimizer): \n",
    "    parameter_gradients,loss_value = grads(N,X,X0,Xb)\n",
    "    optimizer.apply_gradients(zip(parameter_gradients,N.trainable_variables))\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7987f71d-274c-4c56-b28a-b9153522f615",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nepochs = 1000\n",
    "Nchange = 500\n",
    "Nint = 8000\n",
    "N0 = 500\n",
    "Nb = 500\n",
    "Nprint = 100\n",
    "\n",
    "k1 = 1\n",
    "k2 = 1\n",
    "rad_args = (k1, k2)\n",
    "\n",
    "epochs_adam = np.arange(100, Nepochs + 100, 100)\n",
    "loss_list = np.zeros(len(epochs_adam))\n",
    "X = generate_inputs(Nint)\n",
    "X0 = initial_points(N0)\n",
    "Xb = boundary_points(Nb)\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(5e-3, 1000, 0.98)\n",
    "optimizer = Adam(lr, 0.99, 0.999, epsilon=1e-20)\n",
    "\n",
    "start_time = perf_counter()\n",
    "adam_epochs = np.arange(1, Nepochs + 1)\n",
    "adam_loss_list = []\n",
    "\n",
    "for i in range(Nepochs):\n",
    "    if (i + 1) % Nchange == 0:\n",
    "        X = adaptive_rad(N, Nint, rad_args)\n",
    "        X0 = initial_points(N0)\n",
    "        Xb = boundary_points(Nb)\n",
    "\n",
    "    _, fu = get_results(N, X)\n",
    "    u0 = uinit(X0)\n",
    "    u0pinn = output(N, X0)\n",
    "    ul = uleft(Xb)\n",
    "    ulpinn = output(N, Xb[Xb[:, 1] == x0])\n",
    "    ur = uright(Xb)\n",
    "    urpinn = output(N, Xb[Xb[:, 1] == xf])\n",
    "    loss_value = loss(fu, u0, u0pinn, ul, ulpinn, ur, urpinn)\n",
    "    adam_loss_list.append(loss_value.numpy())\n",
    "\n",
    "    training(N, X, X0, Xb, optimizer)\n",
    "\n",
    "adam_end_time = perf_counter()\n",
    "adam_training_time = adam_end_time - start_time\n",
    "print(f\"Total Adam training time: {adam_training_time:.2f} seconds\")\n",
    "\n",
    "with open(\"adam_training_time_burger.txt\", \"w\") as f:\n",
    "    f.write(f\"Adam Training Time: {adam_training_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# --- Utilities for BFGS (no validation reporting) ---\n",
    "def nested_tensor(grad, layer_dims, train_activations=False, bias=True):\n",
    "    if train_activations == False:\n",
    "        if bias:\n",
    "            temp = [None] * (2 * len(layer_dims) - 2)\n",
    "        else:\n",
    "            temp = [None] * (2 * len(layer_dims) - 3)\n",
    "        index = 0\n",
    "        for i in range(len(temp)):\n",
    "            if i % 2 == 0:\n",
    "                expected_shape = (layer_dims[i // 2], layer_dims[i // 2 + 1])\n",
    "                grad_slice_size = layer_dims[i // 2] * layer_dims[i // 2 + 1]\n",
    "                temp[i] = np.reshape(\n",
    "                    grad[index:index + layer_dims[i // 2] * layer_dims[i // 2 + 1]],\n",
    "                    (layer_dims[i // 2], layer_dims[i // 2 + 1]),\n",
    "                )\n",
    "                index += layer_dims[i // 2] * layer_dims[i // 2 + 1]\n",
    "            else:\n",
    "                temp[i] = grad[index:index + layer_dims[i - i // 2]]\n",
    "                index += layer_dims[i - i // 2]\n",
    "        return temp\n",
    "    else:\n",
    "        temp = [None] * (3 * len(layer_dims) - 4)\n",
    "        index = 0\n",
    "        for i in range(len(temp)):\n",
    "            if i % 3 == 0:\n",
    "                temp[i] = np.reshape(\n",
    "                    grad[index:index + layer_dims[i // 3] * layer_dims[i // 3 + 1]],\n",
    "                    (layer_dims[i // 3], layer_dims[i // 3 + 1]),\n",
    "                )\n",
    "                index += layer_dims[i // 3] * layer_dims[i // 3 + 1]\n",
    "            elif i % 3 == 1:\n",
    "                temp[i] = grad[index:index + layer_dims[int((i + 2) / 3)]]\n",
    "                index += layer_dims[int((i + 2) / 3)]\n",
    "            else:\n",
    "                temp[i] = grad[index]\n",
    "                index += 1\n",
    "        return temp\n",
    "\n",
    "\n",
    "power = 1.0\n",
    "\n",
    "@tf.function(jit_compile=True)\n",
    "def loss_and_gradient_TF(N, X, X0, Xb):\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, fu = get_results(N, X)\n",
    "        u0 = uinit(X0)\n",
    "        u0pinn = output(N, X0)\n",
    "        ul = uleft(Xb)\n",
    "        ulpinn = output(N, Xb[Xb[:, 1] == x0])\n",
    "        ur = uright(Xb)\n",
    "        urpinn = output(N, Xb[Xb[:, 1] == xf])\n",
    "        loss_value = loss(fu, u0, u0pinn, ul, ulpinn, ur, urpinn) ** (1.0 / power)\n",
    "    gradsN = tape.gradient(loss_value, N.trainable_variables)\n",
    "    return loss_value, gradsN\n",
    "\n",
    "\n",
    "def loss_and_gradient(weights, N, X, X0, Xb, layer_dims):\n",
    "    resh_weights = nested_tensor(weights, layer_dims)\n",
    "    N.set_weights(resh_weights)\n",
    "    loss_value, grads = loss_and_gradient_TF(N, X, X0, Xb)\n",
    "    grads_flat = np.concatenate([tf.reshape(g, [-1]).numpy() for g in grads])\n",
    "    return loss_value.numpy(), grads_flat\n",
    "\n",
    "\n",
    "Nbfgs = 50000\n",
    "Nbatches = int(round(Nbfgs / Nchange))\n",
    "epochs_bfgs = np.arange(0, Nbfgs + 100, 100)\n",
    "epochs_bfgs += Nepochs\n",
    "lossbfgs = np.zeros(len(epochs_bfgs))\n",
    "Nprint = 100\n",
    "\n",
    "initial_weights = np.concatenate([tf.reshape(w, [-1]).numpy() for w in N.weights])\n",
    "\n",
    "cont = 0\n",
    "time_ellapsed = np.array([])\n",
    "initial_time = perf_counter()\n",
    "\n",
    "def callback(*, intermediate_result):\n",
    "    global cont, lossbfgs, Nprint\n",
    "    if (cont + 1) % Nprint == 0 or cont == 0:\n",
    "        loss_value = intermediate_result.fun ** power\n",
    "        lossbfgs[(cont + 1) // Nprint] = loss_value\n",
    "        print(f\"Iteration {cont + 1}: Train Loss = {loss_value}\")\n",
    "    cont += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af05f912-8039-4985-b351-4dd80285bf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0 = tf.eye(len(initial_weights), dtype=tf.float64).numpy()\n",
    "method = \"BFGS\"\n",
    "method_bfgs = \"SSBFGS_AB\"\n",
    "initial_scale = False\n",
    "\n",
    "initial_time_bfgs = perf_counter()\n",
    "\n",
    "while cont < Nbfgs:\n",
    "    print(cont)\n",
    "    result = minimize(\n",
    "        loss_and_gradient,\n",
    "        initial_weights,\n",
    "        args=(N, X, X0, Xb, layer_dims),\n",
    "        method=method,\n",
    "        jac=True,\n",
    "        options={\n",
    "            \"maxiter\": Nchange,\n",
    "            \"gtol\": 0,\n",
    "            \"hess_inv0\": H0,\n",
    "            \"method_bfgs\": method_bfgs,\n",
    "            \"initial_scale\": initial_scale,\n",
    "        },\n",
    "        tol=0,\n",
    "        callback=callback,\n",
    "    )\n",
    "\n",
    "    initial_weights = result.x\n",
    "    H0 = result.hess_inv\n",
    "    H0 = 0.5 * (H0 + H0.T)\n",
    "\n",
    "    try:\n",
    "        cholesky(H0)\n",
    "    except LinAlgError:\n",
    "        H0 = tf.eye(len(initial_weights), dtype=tf.float64).numpy()\n",
    "\n",
    "    X = adaptive_rad(N, Nint, rad_args)\n",
    "    X0 = initial_points(N0)\n",
    "    Xb = boundary_points(Nb)\n",
    "\n",
    "    _, fu = get_results(N, X)\n",
    "    u0 = uinit(X0)\n",
    "    u0pinn = output(N, X0)\n",
    "\n",
    "    mask_left = tf.equal(Xb[:, 1], tf.cast(x0, Xb.dtype))\n",
    "    mask_right = tf.equal(Xb[:, 1], tf.cast(xf, Xb.dtype))\n",
    "    ul = uleft(Xb)\n",
    "    ulpinn = output(N, tf.boolean_mask(Xb, mask_left))\n",
    "    ur = uright(Xb)\n",
    "    urpinn = output(N, tf.boolean_mask(Xb, mask_right))\n",
    "\n",
    "    loss_value = loss(fu, u0, u0pinn, ul, ulpinn, ur, urpinn)\n",
    "    initial_scale = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b5f69a-7db8-47fe-8498-1b2a4406a858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_env)",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
