{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b58a1f-2044-4e5b-a2a9-d64c7f1997ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "from collections.abc import Callable\n",
    "from typing import TypeVar\n",
    "\n",
    "import numpy as np\n",
    "from pyDOE import lhs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import optimistix as optx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b35c0ab-4cb7-4413-a01b-69fe04bf0b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINN Simulation for 1D Burgers: nu=0.003183098861837907, x∈[-1.0,1.0], t∈[0.0,1.0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'optimistix' has no attribute 'AbstractSSBFGS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPINN Simulation for 1D Burgers: nu=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, x∈[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], t∈[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_min\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt_max\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Optimistix solver wrappers (same idea as your code)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSSBFGSTrustRegion\u001b[39;00m(\u001b[43moptx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAbstractSSBFGS\u001b[49m):\n\u001b[1;32m     44\u001b[0m     rtol: \u001b[38;5;28mfloat\u001b[39m\n\u001b[1;32m     45\u001b[0m     atol: \u001b[38;5;28mfloat\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'optimistix' has no attribute 'AbstractSSBFGS'"
     ]
    }
   ],
   "source": [
    "np.random.seed(1234)\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "dir_path = \"./checkpoints\"\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "\n",
    "nu = 0.01 / np.pi\n",
    "x_min, x_max = -1.0, 1.0\n",
    "t_min, t_max = 0.0, 1.0\n",
    "\n",
    "model_name = \"burgers_1D.eqx\"\n",
    "MODEL_FILE_NAME = os.path.join(dir_path, model_name)\n",
    "\n",
    "\n",
    "\n",
    "class SSBFGSTrustRegion(optx.AbstractSSBFGS):\n",
    "    rtol: float\n",
    "    atol: float\n",
    "    norm: Callable = optx.max_norm\n",
    "    use_inverse: bool = True\n",
    "    search: optx.AbstractSearch = optx.LinearTrustRegion()\n",
    "    descent: optx.AbstractDescent = optx.NewtonDescent()\n",
    "    verbose: frozenset[str] = frozenset()\n",
    "\n",
    "class BroydenTrustRegion(optx.AbstractSSBroyden):                                                                                 \n",
    "    rtol: float \n",
    "    atol: float \n",
    "    norm: Callable = optx.max_norm \n",
    "    use_inverse: bool = True\n",
    "    search: optx.AbstractSearch = optx.LinearTrustRegion()\n",
    "    descent: optx.AbstractDescent = optx.NewtonDescent()\n",
    "    verbose: frozenset[str] = frozenset()  \n",
    "\n",
    "\n",
    "\n",
    "initializer = jax.nn.initializers.glorot_normal()\n",
    "\n",
    "def trunc_init(weight: jax.Array, key: jax.random.PRNGKey) -> jax.Array:\n",
    "    out, in_ = weight.shape\n",
    "    return initializer(key, shape=(out, in_))\n",
    "\n",
    "def init_linear_weight(model, init_fn, key):\n",
    "    is_linear = lambda x_loc: isinstance(x_loc, eqx.nn.Linear)\n",
    "\n",
    "    get_weights = lambda m: [x.weight\n",
    "                             for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "                             if is_linear(x)]\n",
    "    get_bias = lambda m: [x.bias\n",
    "                          for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "                          if is_linear(x)]\n",
    "\n",
    "    weights = get_weights(model)\n",
    "    biases  = get_bias(model)\n",
    "\n",
    "    new_biases = jax.tree_util.tree_map(lambda p: 0.0 * jnp.abs(p), biases)\n",
    "    new_weights = [init_fn(w, sk)\n",
    "                   for w, sk in zip(weights, jax.random.split(key, len(weights)))]\n",
    "\n",
    "    model = eqx.tree_at(get_weights, model, new_weights)\n",
    "    model = eqx.tree_at(get_bias, model, new_biases)\n",
    "    return model\n",
    "\n",
    "\n",
    "class Burgers1D(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key, units=30, depth=4):\n",
    "        keys = jax.random.split(key, depth + 2)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(2, units, key=keys[0]),\n",
    "            *[eqx.nn.Linear(units, units, key=keys[i+1]) for i in range(depth - 1)],\n",
    "            eqx.nn.Linear(units, 1, key=keys[-1]),]\n",
    "\n",
    "    def __call__(self, t, x):\n",
    "        z = jnp.hstack((t, x)) \n",
    "        for layer in self.layers[:-1]:\n",
    "            z = jax.nn.tanh(layer(z))\n",
    "        return self.layers[-1](z)  \n",
    "\n",
    "\n",
    "\n",
    "U0_fn = lambda x: -jnp.sin(jnp.pi * x) \n",
    "Ub_fn = lambda t, x: 0.0              \n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def burgers_residual(net, tt, xx):\n",
    "    u_fn = lambda _t, _x: net(_t, _x)[0]  # scalar\n",
    "\n",
    "    u_t = jax.grad(u_fn, argnums=0)(tt, xx)\n",
    "    u_x = jax.grad(u_fn, argnums=1)(tt, xx)\n",
    "    u_xx = jax.grad(jax.grad(u_fn, argnums=1), argnums=1)(tt, xx)\n",
    "\n",
    "    u = u_fn(tt, xx)\n",
    "    return u_t + u * u_x - nu * u_xx\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss_fn(net, w_data, w_phys, Xf, X0, u0, Xb, ub):\n",
    "    u0_pred = jax.vmap(net, in_axes=(0, 0))(X0[:, 0], X0[:, 1])[:, 0:1]\n",
    "    loss_ic = jnp.mean((u0_pred - u0) ** 2)\n",
    "\n",
    "    ub_pred = jax.vmap(net, in_axes=(0, 0))(Xb[:, 0], Xb[:, 1])[:, 0:1]\n",
    "    loss_bc = jnp.mean((ub_pred - ub) ** 2)\n",
    "\n",
    "    R = jax.vmap(burgers_residual, in_axes=(None, 0, 0))(net, Xf[:, 0], Xf[:, 1]).reshape(-1, 1)\n",
    "    loss_pde = jnp.mean(R ** 2)\n",
    "\n",
    "    loss_data = loss_ic + loss_bc\n",
    "    return w_data * loss_data + w_phys * loss_pde\n",
    "\n",
    "\n",
    "def sample_interior(nf, seed=0):\n",
    "    pts = lhs(2, nf)  # (nf,2)\n",
    "    t = t_min + (t_max - t_min) * pts[:, 0:1]\n",
    "    x = x_min + (x_max - x_min) * pts[:, 1:2]\n",
    "    return jnp.array(np.hstack([t, x]))\n",
    "\n",
    "def sample_initial(n0, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    x = np.random.uniform(x_min, x_max, size=(n0, 1))\n",
    "    t = np.zeros_like(x)\n",
    "    X0 = np.hstack([t, x])\n",
    "    u0 = jax.vmap(U0_fn)(jnp.array(x).reshape(-1)).reshape(-1, 1)\n",
    "    return jnp.array(X0), u0\n",
    "\n",
    "def sample_boundary(nb, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    t = np.random.uniform(t_min, t_max, size=(nb, 1))\n",
    "    # half left, half right\n",
    "    nb_l = nb // 2\n",
    "    nb_r = nb - nb_l\n",
    "    xl = x_min * np.ones((nb_l, 1))\n",
    "    xr = x_max * np.ones((nb_r, 1))\n",
    "    tl = t[:nb_l]\n",
    "    tr = t[nb_l:]\n",
    "\n",
    "    Xbl = np.hstack([tl, xl])\n",
    "    Xbr = np.hstack([tr, xr])\n",
    "    Xb = np.vstack([Xbl, Xbr])\n",
    "\n",
    "    ub = jnp.zeros((nb, 1), dtype=jnp.float64)\n",
    "    return jnp.array(Xb), ub\n",
    "\n",
    "\n",
    "def load(filename, model):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return eqx.tree_deserialise_leaves(f, model)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Nf = 50_000\n",
    "    N0 = 2_000\n",
    "    Nb = 2_000\n",
    "\n",
    "    lambda_data = 5.0   \n",
    "    lambda_phys = 1.0\n",
    "\n",
    "    adam_iters = 10_000\n",
    "    print_every = 200\n",
    "\n",
    "    Xf = sample_interior(Nf, seed=123)\n",
    "    X0, u0 = sample_initial(N0, seed=456)\n",
    "    Xb, ub = sample_boundary(Nb, seed=789)\n",
    "\n",
    "    key = jr.PRNGKey(1234)\n",
    "    key, init_key = jr.split(key)\n",
    "    pinn = Burgers1D(init_key, units=30, depth=4)\n",
    "    pinn = init_linear_weight(pinn, trunc_init, init_key)\n",
    "\n",
    "    lr = 1e-3\n",
    "    optimizer = optax.radam(lr)\n",
    "    opt_state = optimizer.init(eqx.filter(pinn, eqx.is_inexact_array))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def train_step(net, state):\n",
    "        l, g = eqx.filter_value_and_grad(loss_fn)(net, lambda_data, lambda_phys, Xf, X0, u0, Xb, ub)\n",
    "        updates, new_state = optimizer.update(g, state, net)\n",
    "        new_net = eqx.apply_updates(net, updates)\n",
    "        return new_net, new_state, l\n",
    "\n",
    "    def ic_rel_l2(net):\n",
    "        u0_pred = jax.vmap(net, in_axes=(0, 0))(X0[:, 0], X0[:, 1])[:, 0:1]\n",
    "        return float(jnp.linalg.norm(u0_pred - u0) / (jnp.linalg.norm(u0) + 1e-12))\n",
    "\n",
    "    print(\"Training (Adam) started!\")\n",
    "    t0 = time.time()\n",
    "\n",
    "    last_adam_loss = None\n",
    "    last_adam_ic_err = None\n",
    "\n",
    "    for epoch in range(1, adam_iters + 1):\n",
    "        pinn, opt_state, l = train_step(pinn, opt_state)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            last_adam_loss = float(l)\n",
    "            last_adam_ic_err = ic_rel_l2(pinn)\n",
    "            print(f\"[Adam] step {epoch:6d} | loss {last_adam_loss:.3e} | IC relL2 {last_adam_ic_err:.3e}\")\n",
    "\n",
    "    eqx.tree_serialise_leaves(MODEL_FILE_NAME, pinn)\n",
    "    t1 = time.time()\n",
    "    print(f\"Elapsed Time for Adam: {t1 - t0:.2f} sec\")\n",
    "    \n",
    "    params, static = eqx.partition(pinn, eqx.is_inexact_array)\n",
    "\n",
    "    def loss_newton(dynamic_model, static_model):\n",
    "        model = eqx.combine(dynamic_model, static_model)\n",
    "        return loss_fn(model, lambda_data, lambda_phys, Xf, X0, u0, Xb, ub)\n",
    "\n",
    "    solver = SSBFGSTrustRegion(rtol=1e-16, atol=1e-16, verbose=[\"loss\"])\n",
    "    solver = optx.BestSoFarMinimiser(solver)\n",
    "\n",
    "    print(\"Training (SSBFGS) started!\")\n",
    "    t2 = time.time()\n",
    "    sol = optx.minimise(loss_newton, solver, params, args=static, max_steps=100_000, throw=False)\n",
    "    t3 = time.time()\n",
    "\n",
    "    pinn = eqx.combine(sol.value, static)\n",
    "    eqx.tree_serialise_leaves(MODEL_FILE_NAME, pinn)\n",
    "\n",
    "    final_loss = float(loss_fn(pinn, lambda_data, lambda_phys, Xf, X0, u0, Xb, ub))\n",
    "    final_ic_err = ic_rel_l2(pinn)\n",
    "    total_time = (t1 - t0) + (t3 - t2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98d2076-1b04-4fd7-8d04-2827c718cc0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
